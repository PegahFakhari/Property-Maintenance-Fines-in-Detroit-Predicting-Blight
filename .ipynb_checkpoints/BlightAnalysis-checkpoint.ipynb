{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blight Analysis: predicting the probability that the corresponding blight ticket will be paid on time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Blight violations](http://www.detroitmi.gov/How-Do-I/Report/Blight-Complaint-FAQs) are issued by the city to individuals who allow their properties to remain in a deteriorated condition. Every year, the city of Detroit issues millions of dollars in fines to residents and every year, many of these fines remain unpaid (fail to comply with a blight ticket).\n",
    "\n",
    "The target variable is compliance, which is True if the ticket was paid early, on time, or within one month of the hearing data, False if the ticket was paid after the hearing date or not at all, and Null if the violator was found not responsible.\n",
    "\n",
    "\n",
    "All data for this assignment has been provided to us through the [Detroit Open Data Portal](https://data.detroitmi.gov/). \n",
    "\n",
    "**File descriptions** \n",
    "\n",
    "    train.csv - the training set (all tickets issued 2004-2011)\n",
    "    test.csv - the test set (all tickets issued 2012-2016)\n",
    "    addresses.csv & latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = pd.read_csv('train.csv',encoding = 'ISO-8859-1')\n",
    "testData = pd.read_csv('test.csv')\n",
    "address = pd.read_csv('addresses.csv')\n",
    "latlons = pd.read_csv('latlons.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What information do we have and how we can use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data issued by the city of Detroit, we can summarize the description of each column as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ticket_id                                     agency_name  \\\n",
      "0      22056  Buildings, Safety Engineering & Env Department   \n",
      "1      27586  Buildings, Safety Engineering & Env Department   \n",
      "2      22062  Buildings, Safety Engineering & Env Department   \n",
      "3      22084  Buildings, Safety Engineering & Env Department   \n",
      "4      22093  Buildings, Safety Engineering & Env Department   \n",
      "\n",
      "     inspector_name                      violator_name  \\\n",
      "0   Sims, Martinzie  INVESTMENT INC., MIDWEST MORTGAGE   \n",
      "1  Williams, Darrin           Michigan, Covenant House   \n",
      "2   Sims, Martinzie                    SANDERS, DERRON   \n",
      "3   Sims, Martinzie                       MOROSI, MIKE   \n",
      "4   Sims, Martinzie                    NATHANIEL, NEAL   \n",
      "\n",
      "   violation_street_number violation_street_name  violation_zip_code  \\\n",
      "0                   2900.0                 TYLER                 NaN   \n",
      "1                   4311.0               CENTRAL                 NaN   \n",
      "2                   1449.0            LONGFELLOW                 NaN   \n",
      "3                   1441.0            LONGFELLOW                 NaN   \n",
      "4                   2449.0             CHURCHILL                 NaN   \n",
      "\n",
      "   mailing_address_str_number mailing_address_str_name     city     ...      \\\n",
      "0                         3.0                S. WICKER  CHICAGO     ...       \n",
      "1                      2959.0       Martin Luther King  Detroit     ...       \n",
      "2                     23658.0                 P.O. BOX  DETROIT     ...       \n",
      "3                         5.0                ST. CLAIR  DETROIT     ...       \n",
      "4                      7449.0                CHURCHILL  DETROIT     ...       \n",
      "\n",
      "  clean_up_cost judgment_amount payment_amount balance_due  \\\n",
      "0           0.0           305.0            0.0       305.0   \n",
      "1           0.0           855.0          780.0        75.0   \n",
      "2           0.0             0.0            0.0         0.0   \n",
      "3           0.0             0.0            0.0         0.0   \n",
      "4           0.0             0.0            0.0         0.0   \n",
      "\n",
      "          payment_date      payment_status collection_status grafitti_status  \\\n",
      "0                  NaN  NO PAYMENT APPLIED               NaN             NaN   \n",
      "1  2005-06-02 00:00:00        PAID IN FULL               NaN             NaN   \n",
      "2                  NaN  NO PAYMENT APPLIED               NaN             NaN   \n",
      "3                  NaN  NO PAYMENT APPLIED               NaN             NaN   \n",
      "4                  NaN  NO PAYMENT APPLIED               NaN             NaN   \n",
      "\n",
      "                          compliance_detail  compliance  \n",
      "0               non-compliant by no payment         0.0  \n",
      "1  compliant by late payment within 1 month         1.0  \n",
      "2            not responsible by disposition         NaN  \n",
      "3            not responsible by disposition         NaN  \n",
      "4            not responsible by disposition         NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "   ticket_id                      address\n",
      "0      22056       2900 tyler, Detroit MI\n",
      "1      27586     4311 central, Detroit MI\n",
      "2      22062  1449 longfellow, Detroit MI\n",
      "3      22084  1441 longfellow, Detroit MI\n",
      "4      22093   2449 churchill, Detroit MI\n",
      "                                  address        lat        lon\n",
      "0  4300 rosa parks blvd, Detroit MI 48208  42.346169 -83.079962\n",
      "1                14512 sussex, Detroit MI  42.394657 -83.194265\n",
      "2                3456 garland, Detroit MI  42.373779 -82.986228\n",
      "3                5787 wayburn, Detroit MI  42.403342 -82.957805\n",
      "4              5766 haverhill, Detroit MI  42.407255 -82.946295\n"
     ]
    }
   ],
   "source": [
    "print(trainData.head())\n",
    "print(address.head())\n",
    "print(latlons.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the address dataset with the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current information indicating where the resident lives or where where the violation occurred are not helpful. Using the address dataset, I mapped these addresses to longitude and latitude information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-11064 gratiot, Detroit MI</th>\n",
       "      <td>328722</td>\n",
       "      <td>42.406935</td>\n",
       "      <td>-82.995599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-11871 wilfred, Detroit MI</th>\n",
       "      <td>350971</td>\n",
       "      <td>42.411288</td>\n",
       "      <td>-82.993674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-15126 harper, Detroit MI</th>\n",
       "      <td>344821</td>\n",
       "      <td>42.406402</td>\n",
       "      <td>-82.957525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 10th st, Detroit MI</th>\n",
       "      <td>24928</td>\n",
       "      <td>42.325689</td>\n",
       "      <td>-83.064330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0 10th st, Detroit MI</th>\n",
       "      <td>71887</td>\n",
       "      <td>42.325689</td>\n",
       "      <td>-83.064330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ticket_id        lat        lon\n",
       "address                                                    \n",
       "-11064 gratiot, Detroit MI     328722  42.406935 -82.995599\n",
       "-11871 wilfred, Detroit MI     350971  42.411288 -82.993674\n",
       "-15126 harper, Detroit MI      344821  42.406402 -82.957525\n",
       "0 10th st, Detroit MI           24928  42.325689 -83.064330\n",
       "0 10th st, Detroit MI           71887  42.325689 -83.064330"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address = address.set_index('address').join(latlons.set_index('address'),how='left')\n",
    "address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData.set_index('ticket_id').join(address.set_index('ticket_id'))\n",
    "testData = testData.set_index('ticket_id').join(address.set_index('ticket_id'))\n",
    "#trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration is continued: \n",
    "\n",
    "Once I corrected the addresses, I checked the agency_name and insepctor_name to find their unique values. These two factors remain in our features unless the number of the unique values equals to number of observations ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(250306, 35)\n",
      "173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([               u'agency_name',             u'inspector_name',\n",
       "                    u'violator_name',    u'violation_street_number',\n",
       "            u'violation_street_name',         u'violation_zip_code',\n",
       "       u'mailing_address_str_number',   u'mailing_address_str_name',\n",
       "                             u'city',                      u'state',\n",
       "                         u'zip_code',            u'non_us_str_code',\n",
       "                          u'country',         u'ticket_issued_date',\n",
       "                     u'hearing_date',             u'violation_code',\n",
       "            u'violation_description',                u'disposition',\n",
       "                      u'fine_amount',                  u'admin_fee',\n",
       "                        u'state_fee',                   u'late_fee',\n",
       "                  u'discount_amount',              u'clean_up_cost',\n",
       "                  u'judgment_amount',             u'payment_amount',\n",
       "                      u'balance_due',               u'payment_date',\n",
       "                   u'payment_status',          u'collection_status',\n",
       "                  u'grafitti_status',          u'compliance_detail',\n",
       "                       u'compliance',                        u'lat',\n",
       "                              u'lon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(trainData['agency_name'].unique())\n",
    "print(len(trainData['agency_name'].unique()))\n",
    "print(trainData.shape)\n",
    "#print(trainData['inspector_name'].unique())\n",
    "print(len(trainData['inspector_name'].unique()))\n",
    "trainData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the target value from the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating y_train using compliance (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData=trainData.dropna(subset=['compliance'])\n",
    "y_train = trainData['compliance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we focuse on time data that we have in our data:\n",
    "\n",
    "Once a fine ticket is issued, there is window to pay the fine or to file a appeal. In the dataset we have the hearing date and the issue date, so we can calculate this gap (in days). But before that we have to make sure that there is no missing values in these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.hearing_date.fillna(method='pad', inplace=True)\n",
    "testData.hearing_date.fillna(method='pad', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData['gapdays'] = (pd.to_datetime(trainData['hearing_date'])-pd.to_datetime(trainData['ticket_issued_date'])).dt.days\n",
    "testData['gapdays'] = (pd.to_datetime(testData['hearing_date'])-pd.to_datetime(testData['ticket_issued_date'])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the features that are unique to train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_col_to_remove_train = ['payment_amount', 'payment_date','payment_status',\n",
    "                              'balance_due','collection_status', 'compliance_detail',\n",
    "                              'compliance']\n",
    "trainData.drop(list_of_col_to_remove_train, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159880, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61001, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainData.shape)\n",
    "testData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are so many fees in our features, what should we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can either keep the most imporatnt fee, the fine fee, and delete the others or we can add up the fees and create a new column called TotalFee and let the regularization approach decide to keep or omit it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData['TotalFee'] = trainData['admin_fee'] + trainData['state_fee']+\\\n",
    "trainData['late_fee'] - trainData['discount_amount'] + trainData['clean_up_cost']+\\\n",
    "trainData['judgment_amount']\n",
    "\n",
    "testData['TotalFee'] = testData['admin_fee'] + testData['state_fee']+\\\n",
    "testData['late_fee'] - testData['discount_amount'] + testData['clean_up_cost']+\\\n",
    "testData['judgment_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we can remove the unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_col_to_remove_all = ['violation_street_number', 'violation_street_name',\n",
    "                             'violation_zip_code','mailing_address_str_number',\n",
    "                              'mailing_address_str_name','city',\n",
    "                            'state','zip_code', 'non_us_str_code', 'country',\n",
    "                            'violation_description','disposition','violator_name',\n",
    "                            'grafitti_status','ticket_issued_date','hearing_date',\n",
    "                            'admin_fee', 'state_fee', 'late_fee','discount_amount',\n",
    "                             'clean_up_cost', 'judgment_amount','inspector_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData.drop(list_of_col_to_remove_all, axis=1, inplace=True)\n",
    "testData.drop(list_of_col_to_remove_all, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159880, 7)\n",
      "(61001, 7)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape)\n",
    "print(testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([   u'agency_name', u'violation_code',    u'fine_amount',\n",
      "                  u'lat',            u'lon',        u'gapdays',\n",
      "             u'TotalFee'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'agency_name', u'violation_code', u'fine_amount', u'lat', u'lon',\n",
       "       u'gapdays', u'TotalFee'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainData.columns)\n",
    "testData.columns\n",
    "#trainData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By counting the number of observations in the remaining columns, I found that there are some missing values in the latitude and longitude; so fixed them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agency_name       159880\n",
      "violation_code    159880\n",
      "fine_amount       159880\n",
      "lat               159878\n",
      "lon               159878\n",
      "gapdays           159880\n",
      "TotalFee          159880\n",
      "dtype: int64\n",
      "agency_name       61001\n",
      "violation_code    61001\n",
      "fine_amount       61001\n",
      "lat               60996\n",
      "lon               60996\n",
      "gapdays           61001\n",
      "TotalFee          61001\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(trainData.count())\n",
    "print(testData.count())\n",
    "\n",
    "trainData.lat.fillna(method='pad', inplace=True)\n",
    "trainData.lon.fillna(method='pad', inplace=True)\n",
    "\n",
    "testData.lat.fillna(method='pad', inplace=True)\n",
    "testData.lon.fillna(method='pad', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting columns with different levels into factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159880, 233)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(61000, 233)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataT = pd.concat([trainData,testData],axis=0)\n",
    "DataT.shape\n",
    "\n",
    "feature_to_be_splitted = ['violation_code', 'agency_name']\n",
    "DataT = pd.get_dummies(DataT, columns=feature_to_be_splitted)\n",
    "\n",
    "type(DataT)\n",
    "DataT.shape\n",
    "\n",
    "train_data = DataT.iloc[0:trainData.shape[0],]\n",
    "print(train_data.shape)\n",
    "test_data = DataT.iloc[trainData.shape[0]+1:,]\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(trainData['violation_code'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes, Neural Network, Logistic Regression, Random Forest, Gradient Boosting, Support Vector Machines, Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Naive Bayes assumes that features are independent, logistic regression requires there to be little or no multicollinearity among the independent variable! For instance, one of the reasons that I combined the fees was to avoid this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are usually more complex and require more time to fit to the data. With more hidden layers, it gets longer to converge and harder to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caution! I included SVM but only ran it once. It takes time! be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last but not least, I included the dummy classifier as a sanity check! \n",
    "\n",
    "It turns out that the dummy classifier is the worst model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgr = LogisticRegression().fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier().fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLPClassifier(solver='lbfgs', random_state = 0).fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svec = SVC(kernel='linear', C=1).fit(train_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2).fit(train_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_proba = clf.predict_proba(test_data)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_lgr = lgr.predict(test_data)\n",
    "pred_nb = nb.predict(test_data)\n",
    "pred_rf = rf.predict(test_data)\n",
    "pred_gb = gb.predict(test_data)\n",
    "pred_nn = nn.predict(test_data)\n",
    "pred_dummy = dummy_majority.predict(test_data)\n",
    "#pred_svec = svec.predict(test_data)\n",
    "#pred_dt = dt.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy is the default scoring metric\n",
    "print('Cross-validation (accuracy)', cross_val_score(gb, train_data, y_train, cv=5))\n",
    "# use AUC as scoring metric\n",
    "print('Cross-validation (AUC)', cross_val_score(gb, train_data, y_train, cv=5, scoring = 'roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy is the default scoring metric\n",
    "print('Cross-validation (accuracy)', cross_val_score(lgr, train_data, y_train, cv=5))\n",
    "# use AUC as scoring metric\n",
    "print('Cross-validation (AUC)', cross_val_score(lgr, train_data, y_train, cv=5, scoring = 'roc_auc'))\n",
    "np.mean(cross_val_score(lgr, train_data, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy is the default scoring metric\n",
    "print('Cross-validation (accuracy)', cross_val_score(rf, train_data, y_train, cv=5))\n",
    "# use AUC as scoring metric\n",
    "print('Cross-validation (AUC)', cross_val_score(rf, train_data, y_train, cv=5, scoring = 'roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(pred_lgr))\n",
    "print(sum(pred_nb))\n",
    "print(sum(pred_rf))\n",
    "print(sum(pred_gb))\n",
    "print(sum(pred_nn))\n",
    "print(sum(pred_dummy))\n",
    "print(sum(pred_svec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add regularization \n",
    "rocVec = np.zeros(5)\n",
    "for iter,g in enumerate([0.01, 0.1, 1, 10, 100]):\n",
    "    lgr = LogisticRegression(C=g).fit(train_data, y_train)\n",
    "    rocVec[iter] = np.mean(cross_val_score(lgr, train_data, y_train, cv=5, scoring = 'roc_auc'))\n",
    "print(rocVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#blight_model()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "nNS8l",
   "launcher_item_id": "yWWk7",
   "part_id": "w8BSS"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
